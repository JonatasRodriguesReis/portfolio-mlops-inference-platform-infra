apiVersion: v1
kind: ConfigMap
metadata:
  name: inference-latency-dashboard
  namespace: observability
  labels:
    grafana_dashboard: "1"
data:
  inference-latency.json: |
    {
      "title": "Inference API â€“ Latency",
      "uid": "inference-latency",
      "timezone": "browser",
      "schemaVersion": 38,
      "version": 1,
      "refresh": "10s",
      "tags": ["mlops", "latency", "inference"],
      "templating": {
        "list": [
          {
            "name": "namespace",
            "type": "query",
            "datasource": "Prometheus",
            "query": "label_values(inference_request_latency_seconds_count, namespace)",
            "current": {
              "selected": true,
              "value": "mlops-inference-app"
            }
          },
          {
            "name": "app",
            "type": "query",
            "datasource": "Prometheus",
            "query": "label_values(inference_request_latency_seconds_count, app)",
            "current": {
              "selected": true,
              "value": "inference-api"
            }
          }
        ]
      },
      "panels": [
        {
          "type": "timeseries",
          "title": "Latency p50 / p95 / p99",
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "histogram_quantile(0.50, sum(rate(inference_request_latency_seconds_bucket{namespace=\"$namespace\",app=\"$app\"}[1m])) by (le))",
              "legendFormat": "p50",
              "refId": "A"
            },
            {
              "expr": "histogram_quantile(0.95, sum(rate(inference_request_latency_seconds_bucket{namespace=\"$namespace\",app=\"$app\"}[1m])) by (le))",
              "legendFormat": "p95",
              "refId": "B"
            },
            {
              "expr": "histogram_quantile(0.99, sum(rate(inference_request_latency_seconds_bucket{namespace=\"$namespace\",app=\"$app\"}[1m])) by (le))",
              "legendFormat": "p99",
              "refId": "C"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s",
              "min": 0
            }
          },
          "gridPos": { "x": 0, "y": 0, "w": 24, "h": 8 }
        },
        {
          "type": "timeseries",
          "title": "Request Rate (RPS)",
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "sum(rate(inference_requests_total{namespace=\"$namespace\",app=\"$app\"}[1m]))",
              "legendFormat": "RPS",
              "refId": "A"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "reqps",
              "min": 0
            }
          },
          "gridPos": { "x": 0, "y": 8, "w": 12, "h": 6 }
        },
        {
          "type": "stat",
          "title": "Total Requests",
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "sum(inference_requests_total{namespace=\"$namespace\",app=\"$app\"})",
              "refId": "A"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "none"
            }
          },
          "gridPos": { "x": 12, "y": 8, "w": 12, "h": 6 }
        },
        {
          "type": "heatmap",
          "title": "Latency Heatmap",
          "datasource": "Prometheus",
          "targets": [
            {
              "expr": "sum(rate(inference_request_latency_seconds_bucket{namespace=\"$namespace\",app=\"$app\"}[1m])) by (le)",
              "refId": "A"
            }
          ],
          "fieldConfig": {
            "defaults": {
              "unit": "s"
            }
          },
          "gridPos": { "x": 0, "y": 14, "w": 24, "h": 8 }
        }
      ]
    }
